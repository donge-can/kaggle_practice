## 03. What I Learned

> 캐글 커널 커리큘럼을 통해 배운 것을 정리한 것입니다.
>
> 세번째 캐글 커널은 [Home_credit_default_risk](https://www.kaggle.com/c/home-credit-default-risk) 입니다.
>
> binary classification

<br>

### 1. [첫번째 커널](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)

> [본인이 공부한 내용]([https://github.com/donge-can/kaggle_practice/blob/master/03.%20Home%20credit%20default%20risk/03.%20home%20credit_pilsa_01.ipynb](https://github.com/donge-can/kaggle_practice/blob/master/03. Home credit default risk/03. home credit_pilsa_01.ipynb))
>
> 데이터 종류가 많은 캐글이었다.

#### 1.1 format 부동형

```python
'{:.2f}'.format(2/5)
```

- `:` 이걸 넣어야 오류가 안남

<br>

#### 1.2 인사이트 도출

- 분석결과 : younger client일 수록 대출을 잘 갚지 못하는 경향이 있었다.
- 이 때, "younger client를 차별하라는 것이 아니라, younger clients가 잘 pay 할 수 있는 예방책을 만들어야 한다." 는 인사이트를 도출해 냄

<br>

#### 1.3 Feature Engineering

- 처음 캐글 커널이었던 `타이타닉`과 달리, `PolynomialFeatures()` 를 이용해 feature Engineering을 진행했다.

<br>

### 2. [두번째 커널](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)

> [본인이 공부한 내용]([https://github.com/donge-can/kaggle_practice/blob/master/03.%20Home%20credit%20default%20risk/03.%20home%20credit_pilsa_02.ipynb](https://github.com/donge-can/kaggle_practice/blob/master/03. Home credit default risk/03. home credit_pilsa_02.ipynb))
>
> feature engineering 중심으로 보기

#### 2.1 GC

- 메모리 자동으로 관리
- Garbage Collector
- 개발자 입장에서 공부해야 이해가 될 것 같은 부분

<br>

#### 2.2 다중공산성 갖고 있는 변수 제거

- colinear variables
- 독립변수들끼리 너무 높은 관계가 있으면, 모델링 성능에 부정적인 영향을 끼칠 수 있음(다중 공산성 : 독립변수들간의 높은 상관관계가 있을 경우, 모형의 변별력에 영향을 미칠 수 있음)
- 회귀분석의 가정 위배됨(독립변수들은 서로 독립이다)

<br>

#### 2.3 LabelEncoder

<br>

### 3. [세번째 커널](https://www.kaggle.com/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm)

> [본인이 공부한 내용]([https://github.com/donge-can/kaggle_practice/blob/master/03.%20Home%20credit%20default%20risk/03.%20home%20credit_pilsa_03.ipynb](https://github.com/donge-can/kaggle_practice/blob/master/03. Home credit default risk/03. home credit_pilsa_03.ipynb))

#### 3.1 [lightgbm](https://lsjsj92.tistory.com/548)

- boosting 알고리즘 중 하나
- xgboost의 단점(girdSearch로 파라미터 튜닝을 하게 될 경우, 시간이 오래 걸림)을 보완하기 위해 탄생 / XGboost 는 level-wise 하게 늘어나는 방법
- 대용량 데이터 처리가 가능하고, 다른 모델들 보다 더 적은 메모리를 사용함

- LightGBM : leaf-wise  방법으로 트리 분할

![image-20200215135704765](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200215135704765.png)

- [참고](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)  

<br>

#### cf) XGBoost

- GBM의 단점을 보완해줌
- 과적합 방지가 가능한 규제가 포함되어 있다. (L2 규제, L1 규제)
- CART(Classification and Regression Tree)를 기반으로 한다
  - 즉 분류와 회귀 둘다 가능

- 조기 종료 있음
- 가중치 부여를 경사하강법으로 하긴함

#### 3.2 [Catboost](https://dailyheumsi.tistory.com/136)

- 기존의 부스팅 모델이 일괄적으로 모든 훈련 데이터를 대상으로 잔차 계산
  - CatBoost는 일부만 가지고 잔차계산하고, 이걸로 모델을 만드는 과정을 반복
  - 먼저 x1 의 잔차만 계산하고, 이를 기반으로 모델 만듦. 그리고 x2의 잔차를 이 모델로 예측
  - x1, x2의 잔차를 가지고 모델을 만듧. 이를 기반으로 x3, x4의 잔차를 이 모델로 예측
  - x1, x2, x3, x4의 잔차를 가지고 모델을 만듦. 이를 기반으로 x5, x6, x7, x8의 잔차를 모델로 예측
- 단점
  - sparse 한 matrix 는 처리 못함(대부분의 값이 0 인경우 = 희소 행렬)
  - 데이터 대부분이 수치형 변수인 경우, LightGBM 보다 학습 속도가 느림(대부분 범주형 변수인 경우에 사용할 것)

<br>

#### 3.3 pd.factorize() 범주형 -> 숫자 인코딩

```python
sex_encoded , sex_category = data['Sex'].factorize()
```

- 구체적인 내용 [factorize]([https://github.com/donge-can/EDA/blob/master/18.%20pd.factorize.ipynb](https://github.com/donge-can/EDA/blob/master/18. pd.factorize.ipynb))

<br>

#### 3.4 인코딩 

- pd.factorize()
- LabelEncoder()
- One-hot Encoding
- pd.get_dummies()

