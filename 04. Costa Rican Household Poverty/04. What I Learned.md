## 04. What I Learned

> 캐글 커널 커리큘럼을 통해 배운 것을 정리한 것입니다.
>
> 네번째 캐글 커널은 [costa Rican Household poverty level](https://www.kaggle.com/c/costa-rican-household-poverty-prediction)입니다.
>
> 다중 분류

<br>

### 0. 배경 이해

- 가정의 가난 수준 예측

<br>

### 1. [첫번째 커널](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough)

> [본인이 공부한 내용]([https://github.com/donge-can/kaggle_practice/blob/master/04.%20Costa%20Rican%20Household%20Poverty/04.%20Costa%20Rican%20Household%20Poverty_pilsa_01.ipynb](https://github.com/donge-can/kaggle_practice/blob/master/04. Costa Rican Household Poverty/04. Costa Rican Household Poverty_pilsa_01.ipynb))
>
> 다중공산성 등 변수간의 상관관계의 중요성을 다시금 확인

<br>

#### 1.1 `RFECV`

- Recursive Feature Elimination with Cross Validation
- Feature Importance가 작은 순으로 Feature를 제거하면서 Cross Validation score 측정(교차 검정 성능)

- CV를 사용해, 재귀적으로 특성 제거

<br>

#### 1.2 Built-in magic commands

- 주피터 노트북 내에서 사용하는 Magic

```python
%%capture --no-display
```

- [참고](https://ipython.readthedocs.io/en/stable/interactive/magics.html)

<br>

#### 1.3 성능 지표

- `Macro F1`
- ![image-20200302151816670](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200302151816670.png)

  - [scikit-Learn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)
  - 정밀도 Precision : TP/ (TP + FP) , 실제 참인 수 대비 예측이 참
  - 재현율 Recall  : TP/ (TP+FN), 예측이 참인 수 대비 실제 참, 결과가 얼마나 완벽한지에 대한 척도 = 민감도와 같음
    - ![image-20200302152035111](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200302152035111.png)

<br>

#### 1.4 hyperopt

<br>

#### 1.5 OrderedDict

- 순서가 있는 딕셔너리
- [정리]([https://github.com/donge-can/EDA/blob/master/21.%20OrderedDict.ipynb](https://github.com/donge-can/EDA/blob/master/21. OrderedDict.ipynb))

<br>

#### 1.6 cross_val_score

- cv_scores.mean()

<br>

#### 1.7 importances += model.feature_importances_ / nfolds   

<br>

### 2. [두번째 커널](https://www.kaggle.com/youhanlee/3250feats-532-feats-using-shap-lb-0-436)

> [본인이 공부한 내용]([https://github.com/donge-can/kaggle_practice/blob/master/04.%20Costa%20Rican%20Household%20Poverty/04.%20Costa%20Rican%20Household%20Poverty_pilsa_02.ipynb](https://github.com/donge-can/kaggle_practice/blob/master/04. Costa Rican Household Poverty/04. Costa Rican Household Poverty_pilsa_02.ipynb))

<br>

#### 2.1 tqdm

- 상태 진행률 시각적으로 표현

<br>

### 3. [세번째  커널](https://www.kaggle.com/skooch/xgboost)

> [본인이 공부한 내용]([https://github.com/donge-can/kaggle_practice/blob/master/04.%20Costa%20Rican%20Household%20Poverty/04.%20Costa%20Rican%20Household%20Poverty_pilsa_03.ipynb](https://github.com/donge-can/kaggle_practice/blob/master/04. Costa Rican Household Poverty/04. Costa Rican Household Poverty_pilsa_03.ipynb))

<br>

#### 3.1 np.isin()

- np.isin(target, list)
  - 해당 target이 list 내 포함되어 있으면 True, 그렇지 않으면 False 반환
- [정리]([https://github.com/donge-can/EDA/blob/master/22.%20np.isin().ipynb](https://github.com/donge-can/EDA/blob/master/22. np.isin().ipynb))

<br>

#### 3.2 clone

```python
from sklearn.base import clone
```

- model 만 복사하는 함수
- 데이터는 복사하지 않음

```python
for epoch in range(1000):
    sgd_reg.fit(X_train_poly_scaled, y_predict)
    
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    val_error = mean_squared_error(y_predict, y_val_predict)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch =epoch
        
        best_model = clone(sgd_reg)
```

- [참고]([https://hoony-gunputer.tistory.com/entry/%ED%95%B8%EC%A6%88%EC%98%A8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-4%EA%B0%95-%EB%AA%A8%EB%8D%B8%ED%95%99%EC%8A%B5-2%ED%8E%B8](https://hoony-gunputer.tistory.com/entry/핸즈온-머신러닝-4강-모델학습-2편)) 

