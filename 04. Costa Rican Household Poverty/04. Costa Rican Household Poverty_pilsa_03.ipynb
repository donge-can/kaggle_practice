{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 커널 : https://www.kaggle.com/skooch/xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/user/Desktop/kaggle_data/04. costa-rican-household-poverty-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Categorical mappping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "    \n",
    "def feature_importance(forest, X_train, display_results = True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        print('Feature ranking: ')\n",
    "        \n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print('{}. feature {} ({})'.format(f+1, indices[f], importances[indices[f]]) + ' - ' + X_train.columns[indices[f]])\n",
    "            \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0 :\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "               }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "\n",
    "    \n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "\n",
    "   \n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert one hot encoded fields to label encoding\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        #deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy colmn name to be added\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+'train.csv')\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    encode_data(df_) # 인코딩 함수\n",
    "    \n",
    "    return do_features(df_) # FE 함수\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>r4h2</th>\n",
       "      <th>...</th>\n",
       "      <th>agg18_instlevel5_MEAN</th>\n",
       "      <th>agg18_instlevel5_COUNT</th>\n",
       "      <th>agg18_instlevel6_MEAN</th>\n",
       "      <th>agg18_instlevel6_COUNT</th>\n",
       "      <th>agg18_instlevel7_MEAN</th>\n",
       "      <th>agg18_instlevel7_COUNT</th>\n",
       "      <th>agg18_instlevel8_MEAN</th>\n",
       "      <th>agg18_instlevel8_COUNT</th>\n",
       "      <th>agg18_instlevel9_MEAN</th>\n",
       "      <th>agg18_instlevel9_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  r4h1  r4h2  \\\n",
       "0  190000.0       0      3       0     1       1     0    NaN     0     1   \n",
       "1  135000.0       0      4       0     1       1     1    1.0     0     1   \n",
       "2       NaN       0      8       0     1       1     0    NaN     0     0   \n",
       "3  180000.0       0      5       0     1       1     1    1.0     0     2   \n",
       "4  180000.0       0      5       0     1       1     1    1.0     0     2   \n",
       "\n",
       "   ...  agg18_instlevel5_MEAN  agg18_instlevel5_COUNT  agg18_instlevel6_MEAN  \\\n",
       "0  ...                    0.0                     1.0                    0.0   \n",
       "1  ...                    0.0                     1.0                    0.0   \n",
       "2  ...                    1.0                     1.0                    0.0   \n",
       "3  ...                    1.0                     2.0                    0.0   \n",
       "4  ...                    1.0                     2.0                    0.0   \n",
       "\n",
       "   agg18_instlevel6_COUNT  agg18_instlevel7_MEAN  agg18_instlevel7_COUNT  \\\n",
       "0                     1.0                    0.0                     1.0   \n",
       "1                     1.0                    0.0                     1.0   \n",
       "2                     1.0                    0.0                     1.0   \n",
       "3                     2.0                    0.0                     2.0   \n",
       "4                     2.0                    0.0                     2.0   \n",
       "\n",
       "   agg18_instlevel8_MEAN  agg18_instlevel8_COUNT  agg18_instlevel9_MEAN  \\\n",
       "0                    0.0                     1.0                    0.0   \n",
       "1                    1.0                     1.0                    0.0   \n",
       "2                    0.0                     1.0                    0.0   \n",
       "3                    0.0                     2.0                    0.0   \n",
       "4                    0.0                     2.0                    0.0   \n",
       "\n",
       "   agg18_instlevel9_COUNT  \n",
       "0                     1.0  \n",
       "1                     1.0  \n",
       "2                     1.0  \n",
       "3                     2.0  \n",
       "4                     2.0  \n",
       "\n",
       "[5 rows x 218 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>r4h2</th>\n",
       "      <th>...</th>\n",
       "      <th>agg18_instlevel5_MEAN</th>\n",
       "      <th>agg18_instlevel5_COUNT</th>\n",
       "      <th>agg18_instlevel6_MEAN</th>\n",
       "      <th>agg18_instlevel6_COUNT</th>\n",
       "      <th>agg18_instlevel7_MEAN</th>\n",
       "      <th>agg18_instlevel7_COUNT</th>\n",
       "      <th>agg18_instlevel8_MEAN</th>\n",
       "      <th>agg18_instlevel8_COUNT</th>\n",
       "      <th>agg18_instlevel9_MEAN</th>\n",
       "      <th>agg18_instlevel9_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  r4h1  r4h2  \\\n",
       "0       NaN       0      5       0     1       1     0    NaN     1     1   \n",
       "1       NaN       0      5       0     1       1     0    NaN     1     1   \n",
       "2       NaN       0      5       0     1       1     0    NaN     1     1   \n",
       "3       NaN       0     14       0     1       1     1    1.0     0     1   \n",
       "4  175000.0       0      4       0     1       1     1    1.0     0     0   \n",
       "\n",
       "   ...  agg18_instlevel5_MEAN  agg18_instlevel5_COUNT  agg18_instlevel6_MEAN  \\\n",
       "0  ...                    0.0                     2.0                    0.0   \n",
       "1  ...                    0.0                     2.0                    0.0   \n",
       "2  ...                    0.0                     2.0                    0.0   \n",
       "3  ...                    0.0                     1.0                    0.0   \n",
       "4  ...                    1.0                     1.0                    0.0   \n",
       "\n",
       "   agg18_instlevel6_COUNT  agg18_instlevel7_MEAN  agg18_instlevel7_COUNT  \\\n",
       "0                     2.0                    0.0                     2.0   \n",
       "1                     2.0                    0.0                     2.0   \n",
       "2                     2.0                    0.0                     2.0   \n",
       "3                     1.0                    0.0                     1.0   \n",
       "4                     1.0                    0.0                     1.0   \n",
       "\n",
       "   agg18_instlevel8_MEAN  agg18_instlevel8_COUNT  agg18_instlevel9_MEAN  \\\n",
       "0                    0.5                     2.0                    0.5   \n",
       "1                    0.5                     2.0                    0.5   \n",
       "2                    0.5                     2.0                    0.5   \n",
       "3                    1.0                     1.0                    0.0   \n",
       "4                    0.0                     1.0                    0.0   \n",
       "\n",
       "   agg18_instlevel9_COUNT  \n",
       "0                     2.0  \n",
       "1                     2.0  \n",
       "2                     2.0  \n",
       "3                     1.0  \n",
       "4                     1.0  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9557, 218), (23856, 217))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 결측치 제거 및 문자형을 숫자형으로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency는 모두 제곱값으로 바꾸기\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# education == no 는 0으로 바꾸기\n",
    "train.loc[train['edjefa'] == 'no', 'edjefa'] = 0\n",
    "train.loc[train['edjefe'] == 'no', 'edjefe'] = 0\n",
    "test.loc[test['edjefa'] == 'no', 'edjefa'] = 0\n",
    "test.loc[test['edjefe'] == 'no', 'edjefe'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교육 = yes, 이고 가장일 경우에 escolari 로 만들기\n",
    "\n",
    "train.loc[(train['edjefa'] == 'yes') & (train['parentesco1'] == 1), 'edjefa'] = train.loc[(train['edjefa'] == 'yes') & (train['parentesco1'] == 1), 'escolari']\n",
    "train.loc[(train['edjefe'] == 'yes') & (train['parentesco1'] == 1), 'edjefe'] = train.loc[(train['edjefe'] == 'yes') & (train['parentesco1'] == 1), 'escolari']\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edjefa == yes를 - > 숫자 4로\n",
    "\n",
    "train.loc[train['edjefa'] == 'yes', 'edjefa'] = 4\n",
    "train.loc[train['edjefe'] == 'yes', 'edjefe'] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == 'yes', 'edjefa'] = 4\n",
    "test.loc[test['edjefe'] == 'yes', 'edjefe'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['edjefe'] = train['edjefe'].astype(int)\n",
    "train['edjefa'] = train['edjefa'].astype(int)\n",
    "test['edjefe'] = test['edjefe'].astype(int)\n",
    "test['edjefa'] = test['edjefa'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['edjef'] = np.max(train[['edjefa', 'edjefe']], axis = 1)\n",
    "test['edjef'] = np.max(test[['edjefa', 'edjefe']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan 값 대체\n",
    "\n",
    "train['v2a1'] = train['v2a1'].fillna(0)\n",
    "test['v2a1'] = test['v2a1'].fillna(0)\n",
    "\n",
    "train['v18q1'] = train['v18q1'].fillna(0)\n",
    "test['v18q1'] = test['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc'] = train['rez_esc'].fillna(0)\n",
    "test['rez_esc'] = test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), 'meaneduc'] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), 'SQBmeaned'] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), 'meaneduc'] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), 'SQBmeaned'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train.v14a == 1) & (train.sanitario1 == 1) & (train.abastaguano == 0), 'v14a'] = 0\n",
    "train.loc[(train.v14a == 1) & (train.sanitario1 == 1) & (train.abastaguano == 0), 'sanitario1'] = 0\n",
    "\n",
    "test.loc[(test.v14a == 1) & (test.sanitario1 == 1) & (test.abastaguano == 0), 'v14a'] = 0\n",
    "test.loc[(test.v14a == 1) & (test.sanitario1 == 1) & (test.abastaguano == 0), 'sanitario1'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    \n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "    \n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_ = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis = 1)\n",
    "    \n",
    "    del xx, xx_func\n",
    "    return train_, test_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar'] + cols_nums)], pd.get_dummies(df_[cols_2_ohe], columns = cols_2_ohe)], axis =1)\n",
    "    \n",
    "    geo_agg = tmp_df.groupby(['lugar_LE', 'idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how = 'left', on = 'lugar_LE')\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(train)\n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated columns\n",
    "\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female']\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis = 1)\n",
    "test = test.drop(needless_cols, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Split the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight = None, households = None, test_percentage = 0.20, seed = None):\n",
    "    train2 = train.copy()\n",
    "    \n",
    "    cv_hhs = np.random.choice(households, size = int(len(households) * test_percentage), replace = False)\n",
    "    \n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "    \n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis = 1)\n",
    "\n",
    "np.random.seed(seed = None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size = int(len(households) * 0.15), replace = False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + ['idhogar', 'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Fit a voting classifier\n",
    "- Vote based on LGBM models with early stopping based on macro F1 and decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
    "# 5\n",
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_macroF1_lgb(predictions, truth):\n",
    "    pred_labels = predictions.argmax(axis = 1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average = 'macro')\n",
    "    return ('macroF1', 1-f1)\n",
    "\n",
    "fit_params = {'early_stopping_rounds': 500, \n",
    "             'eval_metric' : evaluate_macroF1_lgb,\n",
    "             'eval_set': [(X_train, y_train), (X_test, y_test)],\n",
    "             'verbose' : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y ,sample_weight = None, threshold = True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households = train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households = train_households)\n",
    "        \n",
    "    fit_params['eval_set'] = [(X_test, y_test)]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight = y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average = 'macro')\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average = 'macro')\n",
    "        print('Train F1: ', best_train)\n",
    "        print('Test F1: ', best_cv)\n",
    "        \n",
    "    if threshold:\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "        \n",
    "        else:\n",
    "            print('Unacceptable!!!! Trying again ...')\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight = sample_weight, **fit_params)\n",
    "        \n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    def fit(self, X, y, sample_weight = None, threshold = True, **fit_params):\n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output', 'classification is not supported')\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError('Voting must be \"soft\" or \"hard\"; got (voting = {})'.format( self.voting))\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid \"estimators\" attribute, \"estimators\"'\n",
    "                                ' should be a list of (string, estimator)'\n",
    "                                'tuples')\n",
    "        if (self.weights is not None and len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                            '; got {} weights, {} estimators'.format(len(self.weights), len(self.estimators)))\n",
    "            \n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "        \n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is'\n",
    "                            'required to be a classifier!')\n",
    "            \n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        transformed_y = self.le_.transform(y)\n",
    "        \n",
    "        self.estimators_ = Parallel(n_jobs = self.n_jobs)(delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                                                          sample_weight = sample_weight, threshold = threshold, **fit_params)\n",
    "                                                          for clf in clfs if clf is not None)\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.43771\tvalidation_0-macroF1:0.614644\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.567672\n",
      "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.578871\n",
      "[150]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.576036\n",
      "[200]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.572118\n",
      "[250]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.579058\n",
      "[299]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.57495\n",
      "Train F1:  0.9052797369926351\n",
      "Test F1:  0.43767296281004997\n",
      "[0]\tvalidation_0-merror:0.457912\tvalidation_0-macroF1:0.636377\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.587039\n",
      "[100]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.582981\n",
      "[150]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.592431\n",
      "[200]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.585941\n",
      "[250]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.581433\n",
      "[299]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.581374\n",
      "Train F1:  0.9250425252367706\n",
      "Test F1:  0.4213472723511933\n",
      "[0]\tvalidation_0-merror:0.456229\tvalidation_0-macroF1:0.647114\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.567629\n",
      "[100]\tvalidation_0-merror:0.382155\tvalidation_0-macroF1:0.569168\n",
      "[150]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.567562\n",
      "[200]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.570145\n",
      "[250]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.570524\n",
      "[299]\tvalidation_0-merror:0.375421\tvalidation_0-macroF1:0.562698\n",
      "Train F1:  0.9123264370877074\n",
      "Test F1:  0.44550376930328484\n",
      "[0]\tvalidation_0-merror:0.5\tvalidation_0-macroF1:0.677457\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.575225\n",
      "[100]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.581942\n",
      "[150]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.576609\n",
      "[200]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.5655\n",
      "[250]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.558785\n",
      "[299]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.558379\n",
      "Train F1:  0.9297471752810545\n",
      "Test F1:  0.4431936478785563\n",
      "[0]\tvalidation_0-merror:0.469697\tvalidation_0-macroF1:0.655137\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.595512\n",
      "[100]\tvalidation_0-merror:0.382155\tvalidation_0-macroF1:0.586262\n",
      "[150]\tvalidation_0-merror:0.382155\tvalidation_0-macroF1:0.593165\n",
      "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.581245\n",
      "[250]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.576677\n",
      "[299]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.577452\n",
      "Train F1:  0.9193218582255729\n",
      "Test F1:  0.4270990942776331\n",
      "[0]\tvalidation_0-merror:0.459596\tvalidation_0-macroF1:0.659117\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.582489\n",
      "[100]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.58438\n",
      "[150]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.590863\n",
      "[200]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.591328\n",
      "[250]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.598401\n",
      "[299]\tvalidation_0-merror:0.375421\tvalidation_0-macroF1:0.601606\n",
      "Train F1:  0.8996926965714231\n",
      "Test F1:  0.43405550212941335\n",
      "[0]\tvalidation_0-merror:0.47138\tvalidation_0-macroF1:0.655895\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.62323\n",
      "[100]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.620424\n",
      "[150]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.61666\n",
      "[200]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.619213\n",
      "[250]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.611774\n",
      "[299]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.624393\n",
      "Train F1:  0.9237764470834268\n",
      "Test F1:  0.3927978893318852\n",
      "[0]\tvalidation_0-merror:0.469697\tvalidation_0-macroF1:0.68108\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.611891\n",
      "[100]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.609598\n",
      "[150]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.615502\n",
      "[200]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.61658\n",
      "[250]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.612071\n",
      "[299]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.61267\n",
      "Train F1:  0.8983672318485191\n",
      "Test F1:  0.40512404722284007\n",
      "[0]\tvalidation_0-merror:0.424242\tvalidation_0-macroF1:0.604303\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.361953\tvalidation_0-macroF1:0.561068\n",
      "[100]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.567417\n",
      "[150]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.576656\n",
      "[200]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.563666\n",
      "[250]\tvalidation_0-merror:0.358586\tvalidation_0-macroF1:0.565483\n",
      "[299]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.558967\n",
      "Train F1:  0.9019517888809105\n",
      "Test F1:  0.44554876246164593\n",
      "[0]\tvalidation_0-merror:0.444444\tvalidation_0-macroF1:0.623663\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.622568\n",
      "[100]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.620505\n",
      "[150]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.617336\n",
      "[200]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.617378\n",
      "[250]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.614473\n",
      "[299]\tvalidation_0-merror:0.346801\tvalidation_0-macroF1:0.613611\n",
      "Train F1:  0.9090250127838595\n",
      "Test F1:  0.3946415121746522\n",
      "[0]\tvalidation_0-merror:0.47138\tvalidation_0-macroF1:0.65976\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.400673\tvalidation_0-macroF1:0.611663\n",
      "[100]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.607111\n",
      "[150]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.611603\n",
      "[200]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.604423\n",
      "[250]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.599689\n",
      "[299]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.598011\n",
      "Train F1:  0.8849862434666512\n",
      "Test F1:  0.40925532638325696\n",
      "[0]\tvalidation_0-merror:0.461279\tvalidation_0-macroF1:0.666123\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.594744\n",
      "[100]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.596221\n",
      "[150]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.602945\n",
      "[200]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.60341\n",
      "[250]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.593754\n",
      "[299]\tvalidation_0-merror:0.375421\tvalidation_0-macroF1:0.595885\n",
      "Train F1:  0.8940121572697404\n",
      "Test F1:  0.4175097258272348\n",
      "[0]\tvalidation_0-merror:0.474747\tvalidation_0-macroF1:0.66296\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.594613\n",
      "[100]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.590772\n",
      "[150]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.594742\n",
      "[200]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.589454\n",
      "[250]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.587828\n",
      "[299]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.586648\n",
      "Train F1:  0.8784555309619573\n",
      "Test F1:  0.43760226783021694\n",
      "[0]\tvalidation_0-merror:0.488215\tvalidation_0-macroF1:0.659458\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.405724\tvalidation_0-macroF1:0.599743\n",
      "[100]\tvalidation_0-merror:0.39899\tvalidation_0-macroF1:0.59441\n",
      "[150]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.589681\n",
      "[200]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.592063\n",
      "[250]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.597969\n",
      "[299]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.601112\n",
      "Train F1:  0.9204661399966624\n",
      "Test F1:  0.42117566344374846\n",
      "[0]\tvalidation_0-merror:0.439394\tvalidation_0-macroF1:0.617001\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.591771\n",
      "[100]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.585576\n",
      "[150]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583081\n",
      "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.588309\n",
      "[250]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.589612\n",
      "[299]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.595091\n",
      "Train F1:  0.8800411290789634\n",
      "Test F1:  0.4330871624067155\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state = 217+i, n_estimators = 300, learning_rate = 0.15, n_jobs = 4, **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting = 'soft')\n",
    "del(clfs)\n",
    "\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis =1), y_train, sample_weight = y_train_weights, threshold = False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.7938\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9071\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9142\n"
     ]
    }
   ],
   "source": [
    "# params 4- 400 early stop - 15 estimators - l1 used features - weighted\n",
    "\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis =1)), average = 'macro')\n",
    "\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis = 1)), average = 'macro')\n",
    "\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis =1)), average = 'macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "\n",
    "print(\"Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}\".format(global_score_soft))\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil4_COUNT',\n",
       " 'agg18_estadocivil5_COUNT',\n",
       " 'geo_energcocinar_LE_0',\n",
       " 'geo_epared_LE_2',\n",
       " 'geo_pared_LE_0'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis = 1), display_results = False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking: \n",
      "1. feature 14 (0.01970352604985237) - agg18_escolari_MAX\n",
      "2. feature 125 (0.01817922107875347) - geo_sanitario_LE_2\n",
      "3. feature 58 (0.018052292987704277) - fe_children_fraction\n",
      "4. feature 32 (0.016881752759218216) - agg18_parentesco2_MEAN\n",
      "5. feature 133 (0.01595587283372879) - geo_pared_LE_1\n",
      "6. feature 49 (0.01521733496338129) - edjefe\n",
      "7. feature 15 (0.013887185603380203) - agg18_escolari_MEAN\n",
      "8. feature 1 (0.013547150418162346) - SQBdependency\n",
      "9. feature 47 (0.012796520255506039) - edjef\n",
      "10. feature 124 (0.011644174344837666) - geo_sanitario_LE_1\n",
      "11. feature 112 (0.011553376913070679) - geo_etecho_LE_1\n",
      "12. feature 116 (0.01144295372068882) - geo_elimbasu_LE_0\n",
      "13. feature 90 (0.01123342290520668) - r4t1\n",
      "14. feature 105 (0.010503770783543587) - geo_hogar_total\n",
      "15. feature 52 (0.010420488193631172) - epared_LE\n",
      "16. feature 59 (0.01026556920260191) - fe_human_bed_density\n",
      "17. feature 19 (0.00999092310667038) - agg18_estadocivil2_MEAN\n",
      "18. feature 25 (0.0099242078140378) - agg18_estadocivil5_MEAN\n",
      "19. feature 106 (0.009922844357788563) - geo_bedrooms\n",
      "20. feature 45 (0.009894086048007011) - dependency\n",
      "21. feature 2 (0.009874610230326653) - SQBedjefe\n",
      "22. feature 3 (0.009820040315389633) - SQBescolari\n",
      "23. feature 55 (0.009730815887451172) - etecho_LE\n",
      "24. feature 87 (0.00971975177526474) - r4m1\n",
      "25. feature 76 (0.009653523564338684) - lugar_LE\n",
      "26. feature 6 (0.009653096087276936) - SQBmeaned\n",
      "27. feature 82 (0.009586731903254986) - piso_LE\n",
      "28. feature 100 (0.009456810541450977) - geo_age\n",
      "29. feature 28 (0.00945474673062563) - agg18_parentesco10_MEAN\n",
      "30. feature 7 (0.009268735535442829) - SQBovercrowding\n",
      "31. feature 43 (0.009140120819211006) - cielorazo\n",
      "32. feature 122 (0.009135146625339985) - geo_energcocinar_LE_3\n",
      "33. feature 12 (0.009071401320397854) - agg18_age_MIN\n",
      "34. feature 97 (0.009065071120858192) - tipovivi_LE\n",
      "35. feature 21 (0.008988523855805397) - agg18_estadocivil3_MEAN\n",
      "36. feature 110 (0.008906828239560127) - geo_eviv_LE_2\n",
      "37. feature 85 (0.00888754427433014) - r4h2\n",
      "38. feature 128 (0.008886312134563923) - geo_manual_elec_LE_0\n",
      "39. feature 92 (0.008885974995791912) - refrig\n",
      "40. feature 102 (0.008862542919814587) - geo_dependency\n",
      "41. feature 56 (0.008818298578262329) - eviv_LE\n",
      "42. feature 69 (0.00877202209085226) - fe_working_man_fraction\n",
      "43. feature 67 (0.008579124696552753) - fe_tablet_adult_density\n",
      "44. feature 9 (0.008477356284856796) - age\n",
      "45. feature 60 (0.008471833541989326) - fe_human_density\n",
      "46. feature 104 (0.008452129550278187) - geo_hogar_adul\n",
      "47. feature 0 (0.008433450013399124) - SQBage\n",
      "48. feature 51 (0.008377410471439362) - energcocinar_LE\n",
      "49. feature 96 (0.008295942097902298) - television\n",
      "50. feature 42 (0.008276992477476597) - bedrooms\n",
      "51. feature 61 (0.008256519213318825) - fe_mobile_adult_density\n",
      "52. feature 62 (0.00824511144310236) - fe_mobile_density\n",
      "53. feature 81 (0.008236573077738285) - pared_LE\n",
      "54. feature 111 (0.008210483938455582) - geo_etecho_LE_0\n",
      "55. feature 75 (0.008127727545797825) - hogar_total\n",
      "56. feature 135 (0.00810168869793415) - geo_pared_LE_7\n",
      "57. feature 91 (0.008068579249083996) - r4t2\n",
      "58. feature 107 (0.008024187758564949) - geo_overcrowding\n",
      "59. feature 126 (0.008003108203411102) - geo_sanitario_LE_3\n",
      "60. feature 16 (0.007938525639474392) - agg18_escolari_MIN\n",
      "61. feature 77 (0.007920213043689728) - male\n",
      "62. feature 65 (0.007918000221252441) - fe_rent_per_person\n",
      "63. feature 57 (0.007905316539108753) - fe_all_man_fraction\n",
      "64. feature 54 (0.007814685814082623) - estadocivil_LE\n",
      "65. feature 83 (0.00781099870800972) - qmobilephone\n",
      "66. feature 86 (0.007736428175121546) - r4h3\n",
      "67. feature 4 (0.007721566595137119) - SQBhogar_nin\n",
      "68. feature 94 (0.007711007725447416) - sanitario_LE\n",
      "69. feature 10 (0.007674821652472019) - agg18_age_MAX\n",
      "70. feature 26 (0.0076470281928777695) - agg18_estadocivil6_MEAN\n",
      "71. feature 13 (0.0076317801140248775) - agg18_dis_MEAN\n",
      "72. feature 40 (0.007620023097842932) - area1\n",
      "73. feature 101 (0.007545762229710817) - geo_meaneduc\n",
      "74. feature 11 (0.007508698385208845) - agg18_age_MEAN\n",
      "75. feature 109 (0.007507631089538336) - geo_eviv_LE_1\n",
      "76. feature 79 (0.007505832705646753) - meaneduc\n",
      "77. feature 93 (0.007461387198418379) - rooms\n",
      "78. feature 50 (0.0073648616671562195) - elimbasu_LE\n",
      "79. feature 136 (0.007334062829613686) - bedrooms_to_rooms\n",
      "80. feature 72 (0.007323937956243753) - hogar_adul\n",
      "81. feature 99 (0.007300411816686392) - v2a1\n",
      "82. feature 27 (0.007289784494787455) - agg18_estadocivil7_MEAN\n",
      "83. feature 89 (0.007277860771864653) - r4m3\n",
      "84. feature 129 (0.007271973416209221) - geo_manual_elec_LE_1\n",
      "85. feature 88 (0.007182895205914974) - r4m2\n",
      "86. feature 5 (0.007024974562227726) - SQBhogar_total\n",
      "87. feature 138 (0.0069869402796030045) - tamhog_to_rooms\n",
      "88. feature 17 (0.006951421964913607) - agg18_estadocivil1_COUNT\n",
      "89. feature 114 (0.00689115934073925) - geo_epared_LE_1\n",
      "90. feature 66 (0.006785453762859106) - fe_rent_per_room\n",
      "91. feature 33 (0.006762348115444183) - agg18_parentesco3_MEAN\n",
      "92. feature 46 (0.006571570411324501) - dis\n",
      "93. feature 48 (0.00646447017788887) - edjefa\n",
      "94. feature 117 (0.006348887458443642) - geo_elimbasu_LE_1\n",
      "95. feature 137 (0.005964533891528845) - rent_to_rooms\n",
      "96. feature 23 (0.005941347219049931) - agg18_estadocivil4_MEAN\n",
      "97. feature 120 (0.005906674545258284) - geo_elimbasu_LE_5\n",
      "98. feature 68 (0.005823405459523201) - fe_tablet_density\n",
      "99. feature 84 (0.005803302861750126) - r4h1\n",
      "100. feature 140 (0.0057855406776070595) - r4t3_to_rooms\n",
      "101. feature 78 (0.005715836770832539) - manual_elec_LE\n",
      "102. feature 44 (0.005595833994448185) - computer\n",
      "103. feature 95 (0.005533924791961908) - techo_LE\n",
      "104. feature 74 (0.005404332187026739) - hogar_nin\n",
      "105. feature 144 (0.0049336934462189674) - rent_to_over_18\n",
      "106. feature 143 (0.0047941491939127445) - rent_to_hhsize\n",
      "107. feature 98 (0.004591894336044788) - v18q1\n",
      "108. feature 41 (0.004526375327259302) - area2\n",
      "109. feature 123 (0.004470543470233679) - geo_sanitario_LE_0\n",
      "110. feature 141 (0.004382733721286058) - v2a1_to_r4t3\n",
      "111. feature 18 (0.004321903456002474) - agg18_estadocivil2_COUNT\n",
      "112. feature 37 (0.0042956224642694) - agg18_parentesco7_MEAN\n",
      "113. feature 73 (0.004280555993318558) - hogar_mayor\n",
      "114. feature 63 (0.004137726966291666) - fe_people_not_living\n",
      "115. feature 39 (0.004044017288833857) - agg18_parentesco9_MEAN\n",
      "116. feature 36 (0.00383261451497674) - agg18_parentesco6_MEAN\n",
      "117. feature 29 (0.0037583590019494295) - agg18_parentesco11_MEAN\n",
      "118. feature 53 (0.0034665868151932955) - escolari\n",
      "119. feature 142 (0.0033213330898433924) - hhsize_to_rooms\n",
      "120. feature 35 (0.0030808434821665287) - agg18_parentesco5_MEAN\n",
      "121. feature 30 (0.002976351883262396) - agg18_parentesco12_MEAN\n",
      "122. feature 8 (0.002837259089574218) - abastagua_LE\n",
      "123. feature 34 (0.0017454844200983644) - agg18_parentesco4_MEAN\n",
      "124. feature 71 (0.0017363589722663164) - hacdor\n",
      "125. feature 38 (0.0015210980782285333) - agg18_parentesco8_MEAN\n",
      "126. feature 139 (0.0013559246435761452) - r4t3_to_tamhog\n",
      "127. feature 70 (0.0011075478978455067) - hacapo\n",
      "128. feature 130 (0.0) - geo_manual_elec_LE_3\n",
      "129. feature 134 (0.0) - geo_pared_LE_2\n",
      "130. feature 132 (0.0) - geo_pared_LE_0\n",
      "131. feature 131 (0.0) - geo_manual_elec_LE_4\n",
      "132. feature 127 (0.0) - geo_sanitario_LE_4\n",
      "133. feature 80 (0.0) - overcrowding\n",
      "134. feature 115 (0.0) - geo_epared_LE_2\n",
      "135. feature 113 (0.0) - geo_etecho_LE_2\n",
      "136. feature 121 (0.0) - geo_energcocinar_LE_0\n",
      "137. feature 31 (0.0) - agg18_parentesco1_MEAN\n",
      "138. feature 20 (0.0) - agg18_estadocivil3_COUNT\n",
      "139. feature 119 (0.0) - geo_elimbasu_LE_3\n",
      "140. feature 22 (0.0) - agg18_estadocivil4_COUNT\n",
      "141. feature 103 (0.0) - geo_hogar_nin\n",
      "142. feature 24 (0.0) - agg18_estadocivil5_COUNT\n",
      "143. feature 118 (0.0) - geo_elimbasu_LE_2\n",
      "144. feature 108 (0.0) - geo_eviv_LE_0\n",
      "145. feature 64 (0.0) - fe_people_weird_stat\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_drop_cols.extend(['idhogar', 'parentesco1', 'fe_rent_per_person', 'fe_rent_per_room', 'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1:  0.8973744577845608\n",
      "Test F1:  0.4119287284836981\n",
      "Train F1:  0.8973405328848642\n",
      "Test F1:  0.4044941482768548\n",
      "Train F1:  0.8880139756454893\n",
      "Test F1:  0.39881107834328067\n",
      "Train F1:  0.891769612996069\n",
      "Test F1:  0.43265498149713966\n",
      "Train F1:  0.8921492656245431\n",
      "Test F1:  0.452800720791523\n",
      "Train F1:  0.8938989965907618\n",
      "Test F1:  0.43800331626520606\n",
      "Train F1:  0.8967698827248772\n",
      "Test F1:  0.4109323467334529\n",
      "Train F1:  0.8906362327579868\n",
      "Test F1:  0.45892004679838216\n",
      "Train F1:  0.9006816627520686\n",
      "Test F1:  0.42879021233275155\n",
      "Train F1:  0.8971414961089192\n",
      "Test F1:  0.41035647077305115\n"
     ]
    }
   ],
   "source": [
    "# do the same thing for some extra trees classifiers\n",
    "ets = []    \n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
    "    ets.append(('rf{}'.format(i), rf))   \n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8596\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy : 0.8776\n"
     ]
    }
   ],
   "source": [
    "# w/ threshold, extra drop cols\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis = 1)), average ='macro')\n",
    "\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis =1)), average = 'macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy : {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "\n",
    "for est in vc2.estimators_ : \n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis =1), display_results = False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "        \n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_voters(data, weights = [0.5, 0.5]):\n",
    "    vc.voting = 'soft'\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis =1))\n",
    "    vc2.voting = 'soft'\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis =1))\n",
    "    \n",
    "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    \n",
    "    predictions = np.argmax(final_vote, axis = 1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889950525664811"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights = [0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average = 'macro')\n",
    "global_combo_score_soft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8984463750262703"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights = [0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average = 'macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep = True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis =1))+1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis =1)) +1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep = True)\n",
    "y_subm_ens['Target'] = combine_voters(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "# y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
    "# y_subm_rf.to_csv(sub_file_rf, index=False)\n",
    "# y_subm_ens.to_csv(sub_file_ens, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
